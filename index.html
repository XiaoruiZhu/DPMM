<!DOCTYPE html>
<html>
<head>
  <title>Dirichlet Process Mixture Models</title>
  <meta charset="utf-8">
  <meta name="description" content="Dirichlet Process Mixture Models">
  <meta name="author" content="Xiaorui Zhu">
  <meta name="generator" content="slidify" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <link rel="stylesheet" href="libraries/frameworks/io2012/css/default.css" media="all" >
  <link rel="stylesheet" href="libraries/frameworks/io2012/css/phone.css" 
    media="only screen and (max-device-width: 480px)" >
  <link rel="stylesheet" href="libraries/frameworks/io2012/css/slidify.css" >
  <link rel="stylesheet" href="libraries/highlighters/highlight.js/css/tomorrow.css" />
  <base target="_blank"> <!-- This amazingness opens all links in a new tab. -->  <link rel=stylesheet href="libraries/widgets/bootstrap/css/bootstrap.css"></link>
<link rel=stylesheet href="libraries/widgets/quiz/css/demo.css"></link>
<link rel=stylesheet href="libraries/widgets/interactive/css/aceeditor.css"></link>
<link rel=stylesheet href="libraries/widgets/nvd3/css/nv.d3.css"></link>
<link rel=stylesheet href="libraries/widgets/nvd3/css/rNVD3.css"></link>
<link rel=stylesheet href="./assets/css/ribbons.css"></link>

  
  <!-- Grab CDN jQuery, fall back to local if offline -->
  <script src="http://ajax.aspnetcdn.com/ajax/jQuery/jquery-1.7.min.js"></script>
  <script>window.jQuery || document.write('<script src="libraries/widgets/quiz/js/jquery.js"><\/script>')</script> 
  <script data-main="libraries/frameworks/io2012/js/slides" 
    src="libraries/frameworks/io2012/js/require-1.0.8.min.js">
  </script>
  
  <script src="libraries/widgets/nvd3/js/jquery-1.8.2.min.js"></script>
<script src="libraries/widgets/nvd3/js/d3.v3.min.js"></script>
<script src="libraries/widgets/nvd3/js/nv.d3.min-new.js"></script>
<script src="libraries/widgets/nvd3/js/fisheye.js"></script>
<script src="libraries/widgets/highcharts/js/jquery-1.9.1.min.js"></script>
<script src="libraries/widgets/highcharts/js/highcharts.js"></script>
<script src="libraries/widgets/highcharts/js/highcharts-more.js"></script>
<script src="libraries/widgets/highcharts/js/exporting.js"></script>


</head>
<body style="opacity: 0">
  <slides class="layout-widescreen">
    
    <!-- LOGO SLIDE -->
        <slide class="title-slide segue nobackground">
  <aside class="gdbar">
    <img src="assets/img/logo.png">
  </aside>
  <hgroup class="auto-fadein">
    <h1>Dirichlet Process Mixture Models</h1>
    <h2>Models and Inferences</h2>
    <p>Xiaorui Zhu<br/>Business Analytics</p>
  </hgroup>
  <article></article>  
</slide>
    

    <!-- SLIDES -->
    <slide class="" id="slide-1" style="background:;">
  <article data-timings="">
    
  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-2" style="background:;">
  <hgroup>
    <h2>Contents</h2>
  </hgroup>
  <article data-timings="">
    <ol>
<li>Stick-Breaking and Chinese Restaurant Process</li>
<li>Dirichlet Process Mixture Models</li>
<li>Conjugate Prior</li>
<li>Gibbs Sampling Algorithms</li>
<li>Simulation results</li>
</ol>

<!-- --- &radio -->

<!-- ## Who has higher creativity? -->

<!-- Who has higher creativity? -->

<!-- 1. Man -->

<!-- 2. Woman -->

<!-- 3. Engineer -->

<!-- 4. Artist -->

<!-- *** .hint -->

<!-- Creativity Diversity -->

<!-- *** .explanation -->

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-3" style="background:;">
  <hgroup>
    <h2>Dirichlet Process</h2>
  </hgroup>
  <article data-timings="">
    <p>\(DP\) is a random measure defined as: \[\mu = \sum^{\infty}_{n=1} p_n \delta_{\phi_n}, \] where:</p>

<ul>
<li>\((p_n)_{n\in N}\) are random weights by stick-breaking construction with parameter \(\theta\)</li>
<li>and \((\phi_n)_{n\in N} \overset{iid}{\sim} G_0\) is &quot;the base measure&quot;. </li>
</ul>

<p>Therefore, \(\mu \sim DP(\theta, G_0)\) has following repressentation: </p>

<p>\[\begin{array}
  {rl}
  \mu & = \;  \displaystyle \sum^{\infty}_{i=1} \Big[ V_i \prod^{i-1}_{j=1}(1-V_j) \Big] \delta_{\phi_i} \\
  V_i & \overset{iid}{\sim} \; Beta(1, \theta) \\
  \phi_i & \overset{iid}{\sim} \; G_0
  \end{array}\]</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-4" style="background:;">
  <hgroup>
    <h2>Stick-Breaking construction</h2>
  </hgroup>
  <article data-timings="">
    <p>Let \((V_n)_{n\in N}\) be i.i.d. \(\text{Beta}(1,\theta)\) random variables.</p>

<p>That is, \(P(V_1\in dx) = \theta (1 − x)^{\theta−1} \textbf{1}_{{x\in (0,1)} }dx.\)</p>

<p>Consider:</p>

<p>\[\begin{array}
  {rl}
  P_1 & := \;  V_1 \\
  P_2 & := (1-V_1)V_2 \\
  P_3 & := (1-V_1)(1-V_2)V_3 \\
      & \vdots \\
  P_{n+1} & := \displaystyle V_n \prod^{n-1}_{j=1}(1-V_j)
  \end{array}\]</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-5" style="background:;">
  <hgroup>
    <h2>Stick-Breaking construction</h2>
  </hgroup>
  <article data-timings="">
    <!-- <center>![SB](figure/SB.png)</center> -->

<p><center><img width=800px height=700px src="figure/SB.png"></img></center></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-6" style="background:;">
  <hgroup>
    <h2>Chinese Restaurant Process (CRP)</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>Imagine a Chinese restaurant that has unlimited number of tables.</li>
<li>First customer sits at the first table.</li>
<li>Customer \(n\) sits at: 

<ul>
<li>Table \(k\) with probability \(n_k/(\alpha_0+n−1)\), where \(n_k\) is the number of customers
at table \(k\).</li>
<li>A new table \(k + 1\) with probability \(\alpha_0/(\alpha_0+n−1)\)</li>
</ul></li>
</ul>

<p>In this metaphor, customers are analogies of integers and tables of clusters. This process can also be summarized as follows:
\[p(c_n=k|c_{1:(n-1)}) = \{ \begin{array}
  {l}
  \frac{n_k}{\alpha_0+n−1 },  \; \text{if occupied table;} \\
  \frac{\alpha_0}{\alpha_0+n−1}, \; \text{if new table} 
  \end{array} \]</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-7" style="background:;">
  <hgroup>
    <h2>Animation of (CRP)</h2>
  </hgroup>
  <article data-timings="">
    <!-- <center>![CRP](figure/example_1.gif)</center> -->

<p><center><img width=500px height=500px src="figure/example_1.gif"></img></center></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-8" style="background:;">
  <hgroup>
    <h2>Simulation of Asymptotics</h2>
  </hgroup>
  <article data-timings="">
    <p>Asymptotics of \(K_n\): Number of clusters</p>

<ul>
<li><strong>Theorem:</strong> \(\displaystyle \text{lim}_{n\rightarrow\infty}K_n/\text{log}n = \theta\) almost surely.</li>
</ul>

<p><img src="assets/fig/unnamed-chunk-1-1.png" title="plot of chunk unnamed-chunk-1" alt="plot of chunk unnamed-chunk-1" style="display: block; margin: auto;" /></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-9" style="background:;">
  <hgroup>
    <h2>Simulation of Asymptotics</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><strong>Theorem:</strong> Asymptotic distribution of \(K_n\): \[\frac{K_n-\mathbb{E}K_n}{\sqrt{\text{Var}(K_n)}} \Rightarrow \mathcal{N}(0,1)\]</li>
</ul>

<p><center><img src="figure/Them10.6.png" alt="Theorem 10.6"></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-10" style="background:;">
  <hgroup>
    <h2>MCMC &amp; Gibbs Sampler</h2>
  </hgroup>
  <article data-timings="">
    <ol>
<li><p><strong>Markov chain Monte Carlo simulation(MCMC):</strong> is a general method based on drawing values of \(\theta\) from approximate distributions and then correcting those draws to better approximate the target posterior distribution, \(p(\theta|y)\). </p></li>
<li><p><strong>Gibbs Sampler:</strong> also called alternating conditional sampling. Each iteration draws each subset conditional on the value of all the others \((\theta = (\theta_1, \cdots , \theta_d))\).</p></li>
</ol>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-11" style="background:;">
  <hgroup>
    <h2>Conjugate Prior is importants</h2>
  </hgroup>
  <article data-timings="">
    <p>If the posterior distributions \(p(\theta|x)\) are in the <strong>same family as the prior probability distribution</strong> \(p(\theta)\) , the prior and posterior are then called conjugate distributions, and the prior is called <strong>a conjugate prior</strong> for the likelihood function.</p>

<p><strong>Model parameter</strong> \(\mu\): mean of Normal with known variance \(\sigma^2\). </p>

<p>Prior of \(\mu\) is \(\mathcal{N}(\mu_0, \sigma^2_0)\)</p>

<p>By derivation, posterior distribution is :</p>

<p>\[\mathcal{N}\Bigg(\Bigg(\frac{1}{\sigma _{0}^{2}}+\frac{n}{\sigma ^{2}} \Bigg)^{-1} \Bigg(\frac{\mu _{0}}{\sigma _{0}^{2}}+\frac{\sum _{i=1}^{n}x_{i}}{\sigma^{2}}\Bigg),\Bigg(\frac {1}{\sigma _{0}^{2}}+\frac{n}{\sigma ^{2}}\Bigg)^{-1}\Bigg)\]</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-12" style="background:;">
  <hgroup>
    <h2>DPMM &amp; Gibbs Sampler Algorithm</h2>
  </hgroup>
  <article data-timings="">
    <p>Simple Mixture Model: \[\begin{array} {l}
y|\theta_i \sim \mathcal{N}(\theta_i, 1) \\
\theta_i \sim G \\
G \sim DP(\alpha, G_0) \\
G_0 \sim \mathcal{N}(0,2) 
\end{array}\]</p>

<ul>
<li><p>Likelihood function: \(F(y_i|\theta) = \frac{1}{\sqrt{2\pi}}e^{\frac{1}{2}(y_i - \theta)^2}\)</p></li>
<li><p>Posterior distribution \(H_i = p(\theta|y_i)= \frac{F(y_i|\theta)G_0(\theta)}{\int{F(y_i|\theta)G_0(\theta)}}= \frac{1}{\sqrt{2\pi}\sqrt{2/3}}e^{\frac{(\theta - \frac{2}{3}y_i)^2}{2 * (2/3)}}\)</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-13" style="background:;">
  <hgroup>
    <h2>DPMM &amp; Gibbs Sampler Algorithm</h2>
  </hgroup>
  <article data-timings="">
    <p>The conditional distribution for Gibbs sampling is as following: </p>

<p>\[\begin{array}
{rl}
\theta^t_{i}|\theta^t_{-i},y_i \sim & \sum_{j\ne i} q_{i,j} \delta(\theta^t_j) + r_i H_i \\
q_{i,j} =                           & b_i F(y_i, \theta_j) \\
r_i =                               & b_i \alpha \int F(y_i, \theta)G_0(\theta) \\
\text{where } b_i \text{ satisfied}                                & \sum_{j\ne i}q_{i,j} + r_i = 1
\end{array}\]</p>

<ul>
<li><p>\(\int{F(y_i|\theta)G_0(\theta)} = \frac{1}{\sqrt{6\pi}}e^{\frac{1}{6}(y_i)^2}\)</p></li>
<li><p>or another simple way: \(\Big(= \frac{F(y_i|\theta)G_0(\theta)}{H_i(\theta|y_i)}\Big)\)</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-14" style="background:;">
  <hgroup>
    <h2>DPMM &amp; Gibbs Sampler Algorithm</h2>
  </hgroup>
  <article data-timings="">
    <p>\[\begin{array}
{ll}
\hline
\textbf{Algorithm:} & \text{Gibbs Sampler for DPMM}  \\
\hline
1.\mathbf{Input:}   & \mathbf{y} \in \mathbb{R}^n,\;  \\
    & \theta_i \in (0,1), i=1,\cdots, n \\
2. \mathbf{Repeat:} & (1) \;  q^*_{i,j} =  F(y_i, \theta_i) \\
                    & (2) \;  r^*_{i} = \alpha \int F(y_i, \theta_i) d G_0(\theta) \\
                    & (3) \;  b_{i} = 1/(\sum^n_{j=1} q^*_{i,j} + r^*_{i} ) \\
                    & (4) \;  \text{Draw} \; \theta^t_{i}|\theta^t_{-i,y_i} \sim \sum_{j\ne i} b_i q^*_{i,j} \delta(\theta^t_j) + b_i r^*_i H_i \\
                    & (5) \;  \text{Update} \; i=1, \cdots, n \\
3. \mathbf{Deliver:} & \hat\theta = \theta^{(t)} \\
\hline
\end{array}\]</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-15" style="background:;">
  <hgroup>
    <h2>Convergency of Algorithms</h2>
  </hgroup>
  <article data-timings="">
    <p>Average total number of clusters \((K_n)\) v.s iteration times \((M)\) of Gibbs Sampler (Algorithm 1). </p>

<p>\((N=100, M\in (2,7,20,54,148,403), \text{Rep}=100)\)</p>

<p><img src="assets/fig/unnamed-chunk-3-1.png" title="plot of chunk unnamed-chunk-3" alt="plot of chunk unnamed-chunk-3" style="display: block; margin: auto;" /></p>

<ul>
<li>Algorithm 1 converge very quick. </li>
<li>When \(M>50\), total number of cluster from Gibbs Sampler is acceptable. </li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-16" style="background:;">
  <hgroup>
    <h2>Convergency of Algorithms</h2>
  </hgroup>
  <article data-timings="">
    <p>Histogram of 100 replications for every given M:</p>

<!-- <center>![Convergency of Algorithm](figure/Covg_M.png) -->

<p><center><img width=600px height=600px src="figure/Covg_M.png"></img></center></p>

<ul>
<li>Total number of clusters approach the truth when M increases (\(n^0_c=10\))</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-17" style="background:;">
  <hgroup>
    <h2>Inference of cluster center</h2>
  </hgroup>
  <article data-timings="">
    <p>Centers of clusters might be of interest to you. </p>

<p><center><img width=400px height=400px src="figure/2BestGibbs.png" align="left"></img>
<img width=400px height=400px src="figure/BestGibbs.png" align="right"></img>
</center></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-18" style="background:;">
  <hgroup>
    <h2>Inference of cluster center</h2>
  </hgroup>
  <article data-timings="">
    <p>Animation of Centers of each cluster (100 simulation): </p>

<p><center><img width=400px height=400px src="figure/AnimatedGibbsCenter.gif"></img></center></p>

  </article>
  <!-- Presenter Notes -->
</slide>

    <slide class="backdrop"></slide>
  </slides>
  <div class="pagination pagination-small" id='io2012-ptoc' style="display:none;">
    <ul>
      <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=1 title='NA'>
         1
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=2 title='Contents'>
         2
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=3 title='Dirichlet Process'>
         3
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=4 title='Stick-Breaking construction'>
         4
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=5 title='Stick-Breaking construction'>
         5
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=6 title='Chinese Restaurant Process (CRP)'>
         6
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=7 title='Animation of (CRP)'>
         7
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=8 title='Simulation of Asymptotics'>
         8
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=9 title='Simulation of Asymptotics'>
         9
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=10 title='MCMC &amp; Gibbs Sampler'>
         10
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=11 title='Conjugate Prior is importants'>
         11
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=12 title='DPMM &amp; Gibbs Sampler Algorithm'>
         12
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=13 title='DPMM &amp; Gibbs Sampler Algorithm'>
         13
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=14 title='DPMM &amp; Gibbs Sampler Algorithm'>
         14
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=15 title='Convergency of Algorithms'>
         15
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=16 title='Convergency of Algorithms'>
         16
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=17 title='Inference of cluster center'>
         17
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=18 title='Inference of cluster center'>
         18
      </a>
    </li>
  </ul>
  </div>  <!--[if IE]>
    <script 
      src="http://ajax.googleapis.com/ajax/libs/chrome-frame/1/CFInstall.min.js">  
    </script>
    <script>CFInstall.check({mode: 'overlay'});</script>
  <![endif]-->
</body>
  <!-- Load Javascripts for Widgets -->
  <script src="libraries/widgets/bootstrap/js/bootstrap.min.js"></script>
<script src="libraries/widgets/bootstrap/js/bootbox.min.js"></script>
<script src="libraries/widgets/quiz/js/jquery.quiz.js"></script>
<script src="libraries/widgets/quiz/js/mustache.min.js"></script>
<script src="libraries/widgets/quiz/js/quiz-app.js"></script>
<script src="libraries/widgets/interactive/js/ace/js/ace.js"></script>
<script src="libraries/widgets/interactive/js/opencpu-0.5.js"></script>
<script src="libraries/widgets/interactive/js/interactive.js"></script>

  <!-- MathJax: Fall back to local if CDN offline but local image fonts are not supported (saves >100MB) -->
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        processEscapes: true
      }
    });
  </script>
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/2.0-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <!-- <script src="https://c328740.ssl.cf1.rackcdn.com/mathjax/2.0-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script> -->
  <script>window.MathJax || document.write('<script type="text/x-mathjax-config">MathJax.Hub.Config({"HTML-CSS":{imageFont:null}});<\/script><script src="libraries/widgets/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"><\/script>')
</script>
<script>  
  $(function (){ 
    $("#example").popover(); 
    $("[rel='tooltip']").tooltip(); 
  });  
  </script>  
  
  <script src="shared/shiny.js" type="text/javascript"></script>
  <script src="shared/slider/js/jquery.slider.min.js"></script>
  <script src="shared/bootstrap/js/bootstrap.min.js"></script>
  <link rel="stylesheet" href="shared/slider/css/jquery.slider.min.css"></link>
  
  <!-- LOAD HIGHLIGHTER JS FILES -->
  <script src="libraries/highlighters/highlight.js/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <!-- DONE LOADING HIGHLIGHTER JS FILES -->
   
  </html>